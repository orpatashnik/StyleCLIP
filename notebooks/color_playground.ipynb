{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4J1lmK2Viqey"
      },
      "source": [
        "# Text-Guided Editing of Images (Using CLIP and StyleGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZzVvsrKivwl",
        "outputId": "2205d533-eaca-46c2-e836-2135a8b58ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'StyleCLIP'...\n",
            "remote: Enumerating objects: 885, done.\u001b[K\n",
            "remote: Counting objects: 100% (267/267), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 885 (delta 169), reused 196 (delta 140), pack-reused 618\u001b[K\n",
            "Receiving objects: 100% (885/885), 241.88 MiB | 20.84 MiB/s, done.\n",
            "Resolving deltas: 100% (314/314), done.\n",
            "Updating files: 100% (260/260), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-j0x2su8n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-j0x2su8n\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=8c89ab146540d538c76dd592384dc327fcae109cc55ba36bdbcce08b157a438a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4r1020xd/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "#@title Setup (may take a few minutes)\n",
        "!git clone https://github.com/khalilacheche/StyleCLIP.git\n",
        "\n",
        "import os\n",
        "os.chdir(f'./StyleCLIP')\n",
        "\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# downloads StyleGAN's weights and facial recognition network weights\n",
        "ids = ['1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT', '1N0MZSqPRJpLfP4mFQCS14ikrVSe8vQlL']\n",
        "for file_id in ids:\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.FetchMetadata(fetch_all=True)\n",
        "  downloaded.GetContentFile(downloaded.metadata['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "XTAVTULlq87j"
      },
      "outputs": [],
      "source": [
        "experiment_type = 'edit' #@param ['edit', 'free_generation']\n",
        "\n",
        "semantic_part = \"hair\" #@param [\"mouth\",\"skin\",\"eyes\",\"nose\",\"ears\",\"eye_brows\",\"hat\",\"hair\",\"neck\"]\n",
        "\n",
        "description = 'A person with ### hair' #@param {type:\"string\"}\n",
        "\n",
        "latent_path = None #@param {type:\"string\"}\n",
        "\n",
        "optimization_steps = 40 #@param {type:\"number\"}\n",
        "\n",
        "clip_lambda = 1 #@param {type:\"number\"}\n",
        "\n",
        "l2_lambda = 0.004 #@param {type:\"number\"}\n",
        "\n",
        "loc_lambda = 0.00001 #@param {type:\"number\"}\n",
        "\n",
        "id_lambda = 0.005 #@param {type:\"number\"}\n",
        "\n",
        "stylespace = False #@param {type:\"boolean\"}\n",
        "\n",
        "create_video = False #@param {type:\"boolean\"}\n",
        "\n",
        "export_segmentation_image = True #@param {type:\"boolean\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = ['black', 'blue', 'red', 'blonde', 'gray', 'purple', 'brown']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "cellView": "form",
        "id": "cuXs6QD8mKjI"
      },
      "outputs": [],
      "source": [
        "use_seed = True #@param {type:\"boolean\"}\n",
        "\n",
        "seed = 1 #@param {type: \"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "cellView": "form",
        "id": "CcBz_eEomF7Q"
      },
      "outputs": [],
      "source": [
        "#@title Additional Arguments\n",
        "args = {\n",
        "    \"description\": description,\n",
        "    \"ckpt\": \"stylegan2-ffhq-config-f.pt\",\n",
        "    \"stylegan_size\": 1024,\n",
        "    \"lr_rampup\": 0.05,\n",
        "    \"lr\": 0.1,\n",
        "    \"step\": optimization_steps,\n",
        "    \"mode\": experiment_type,\n",
        "    \"clip_lambda\": clip_lambda,\n",
        "    \"l2_lambda\": l2_lambda,\n",
        "    \"id_lambda\": id_lambda,\n",
        "    \"loc_lambda\": loc_lambda,\n",
        "    'work_in_stylespace': stylespace,\n",
        "    \"latent_path\": latent_path,\n",
        "    \"truncation\": 0.7,\n",
        "    \"save_intermediate_image_every\": 1 if create_video else 20,\n",
        "    \"results_dir\": \"results\",\n",
        "    \"ir_se50_weights\": \"model_ir_se50.pth\",\n",
        "    \"segmentation_model\": \"face_segmentation\",\n",
        "    \"semantic_part\":semantic_part,\n",
        "    \"export_segmentation_image\": export_segmentation_image\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT9JRl8hnT1l",
        "outputId": "77a66a6f-6908-4288-c52a-41fd1013f8b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ResNet ArcFace\n",
            "Loading Segmentation Models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.9204, loc_loss: 7124.3711;: 100%|██████████| 40/40 [00:33<00:00,  1.18it/s]\n"
          ]
        }
      ],
      "source": [
        "if use_seed:\n",
        "  import torch\n",
        "  torch.manual_seed(seed)\n",
        "from optimization.run_optimization import main\n",
        "from argparse import Namespace\n",
        "from criteria.clip_loss import CLIPLoss\n",
        "import clip\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "clip_loss_evals = {}\n",
        "for color in colors:  \n",
        "  args['description'] = description.replace('###', color)\n",
        "  result = main(Namespace(**args))\n",
        "  img_path = 'results/00059.jpg'\n",
        "  image = Image.open(img_path)\n",
        "  transform = transforms.ToTensor()\n",
        "  img_tensor = transform(image).unsqueeze(0).cuda()\n",
        "  clip_loss = CLIPLoss(Namespace(**args))\n",
        "  clip_losses = {}\n",
        "  for eval_color in colors:\n",
        "    desc = description.replace('###', eval_color)\n",
        "    text_inputs = torch.cat([clip.tokenize(desc)]).cuda()\n",
        "    c_loss = clip_loss(img_tensor, text_inputs)\n",
        "    clip_losses[eval_color] = c_loss\n",
        "  clip_loss_evals[color] = clip_losses\n",
        "\n",
        "print(clip_loss_evals)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "playground.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
